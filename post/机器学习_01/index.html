<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="zh-cn" lang="zh-cn">
<head>
  <link href="//gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="generator" content="Hugo 0.50" />

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>机器学习_01 &middot; 自留地</title>

  
  <link type="text/css" rel="stylesheet" href="https://yunfengsay.github.io/css/print.css" media="print">
  <link type="text/css" rel="stylesheet" href="https://yunfengsay.github.io/css/poole.css">
  <link type="text/css" rel="stylesheet" href="https://yunfengsay.github.io/css/syntax.css">
  <link type="text/css" rel="stylesheet" href="https://yunfengsay.github.io/css/hyde.css">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Abril+Fatface|PT+Sans:400,400i,700">


  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/favicon.png">

  
  <link href="" rel="alternate" type="application/rss+xml" title="自留地" />

  
</head>

  <body class=" ">
  <aside class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <a href="https://yunfengsay.github.io/"><h1>自留地</h1></a>
      <p class="lead">
      
      </p>
    </div>

    <nav>
      <ul class="sidebar-nav">
        <li><a href="https://yunfengsay.github.io/">Home</a> </li>
        
      </ul>
    </nav>

    <p>&copy; 2018. All rights reserved. </p>
  </div>
</aside>

    <main class="content container">
    <div class="post">
  <h1>机器学习_01</h1>
  <time datetime=2018-10-31T10:20:57&#43;0800 class="post-date">Wed, Oct 31, 2018</time>
  

<h3 id="pytorch">pytorch</h3>

<hr />

<h4 id="squeeze-挤-压榨-方法">squeeze(挤，压榨) 方法</h4>

<p>除size为1的维度，包括行和列。至于维度大于等于2时，squeeze()不起作用。</p>

<h4 id="batchsize">batchsize</h4>

<p>批大小。在深度学习中，一般采用SGD训练，即每次训练在训练集中取batchsize个样本训练；</p>

<h4 id="iteration">iteration</h4>

<p>1个iteration等于使用batchsize个样本训练一次；</p>

<h4 id="epoch">epoch</h4>

<p>1个epoch等于使用训练集中的全部样本训练一次；</p>

<pre><code>举个例子，训练集有1000个样本，batchsize=10，那么：
训练完整个样本集需要：
100次iteration，1次epoch
</code></pre>

<h4 id="learning-rate">learning rate</h4>

<p>学习率 (learning rate)，控制 模型的 <strong>学习进度</strong> ：</p>

<p><strong>lr</strong> 即 <strong>stride (步长)</strong> ，即<a href="https://blog.csdn.net/jningwei/article/details/78956190">反向传播算法</a>中的 ηη ：</p>

<p>ωn←ωn−η∂L∂ωn</p>

<table>
<thead>
<tr>
<th>学习率 大</th>
<th>学习率 小</th>
<th></th>
</tr>
</thead>

<tbody>
<tr>
<td>学习速度</td>
<td>快</td>
<td>慢</td>
</tr>

<tr>
<td>使用时间点</td>
<td>刚开始训练时</td>
<td>一定轮数过后</td>
</tr>

<tr>
<td>副作用</td>
<td>1.易损失值爆炸；2.易振荡。</td>
<td>1.易过拟合；2.收敛速度慢。</td>
</tr>
</tbody>
</table>

<h2 id="学习率设置">学习率设置</h2>

<p>在训练过程中，一般根据训练轮数设置动态变化的学习率。</p>

<ul>
<li>刚开始训练时：学习率以 0.01 ~ 0.001 为宜。</li>
<li>一定轮数过后：逐渐减缓。</li>
<li>接近训练结束：学习速率的衰减应该在100倍以上。</li>
</ul>

<p>Note：
如果是 迁移学习 ，由于模型已在原始数据上收敛，此时应设置较小学习率 (≤10−4≤10−4) 在新数据上进行 微调 。</p>

<p>学习率减缓机制
轮数减缓    指数减缓    分数减缓
英文名 step decay  exponential decay   1/t1/t decay
方法  每N轮学习率减半    学习率按训练轮数增长指数插值递减    lrt=lr0/(1+kt)lrt=lr0/(1+kt) ，kk 控制减缓幅度，tt 为训练轮数
把脉 目标函数损失值 曲线
理想情况下 曲线 应该是 滑梯式下降 [绿线]：</p>

<p><img src="https://img.alicdn.com/tfs/TB1z8DVlZfpK1RjSZFOXXa6nFXa-699-335.png" alt="img" /></p>

<p>曲线 初始时 上扬 [红线]：
Solution：初始 学习率过大 导致 振荡，应减小学习率，并 从头 开始训练 。
曲线 初始时 强势下降 没多久 归于水平 [紫线]：
Solution：后期 学习率过大 导致 无法拟合，应减小学习率，并 重新训练 后几轮 。
曲线 全程缓慢 [黄线]：</p>

<h4 id="conv1d-https-keras-cn-readthedocs-io-en-latest-layers-convolutional-layer-层-一维卷积层"><a href="https://keras-cn.readthedocs.io/en/latest/layers/convolutional_layer/">Conv1D</a> 层   一维卷积层</h4>

<pre><code>keras.layers.convolutional.Conv1D(filters, kernel_size, strides=1, padding='valid', dilation_rate=1, activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None)
</code></pre>

<p>一维卷积层（即时域卷积），用以在一维输入信号上进行邻域滤波。当使用该层作为首层时，需要提供关键字参数<code>input_shape</code>。例如<code>(10,128)</code>代表一个长为10的序列，序列中每个信号为128向量。而<code>(None, 128)</code>代表变长的128维向量序列。</p>

<h4 id="conv2d-https-keras-cn-readthedocs-io-en-latest-layers-convolutional-layer-层-二维卷积层"><a href="https://keras-cn.readthedocs.io/en/latest/layers/convolutional_layer/">Conv2D</a> 层 二维卷积层</h4>

<p>二维卷积层，即对图像的空域卷积。该层对二维输入进行滑动窗卷积，当使用该层作为第一层时，应提供<code>input_shape</code>参数。例如<code>input_shape = (128,128,3)</code>代表128*128的彩色RGB图像（<code>data_format='channels_last'</code>）</p>

<h4 id="in-channels-样本通道数-深度">in_channels 样本通道数（深度）</h4>

<h4 id="in-height-样本高度">in_height 样本高度</h4>

<h4 id="batch-就是一个batch的size-比如同时丢100-个样本什么的">batch 就是一个batch的size，比如同时丢100 个样本什么的</h4>

<h4 id="深度学习最全优化方法总结比较-sgd-adagrad-adadelta-adam-adamax-nadam-https-zhuanlan-zhihu-com-p-22252270"><a href="https://zhuanlan.zhihu.com/p/22252270">深度学习最全优化方法总结比较（SGD，Adagrad，Adadelta，Adam，Adamax，Nadam）</a></h4>

<h4 id="view方法">view方法</h4>

<p>相当于numpy中resize（）的功能，但是用法可能不太一样。</p>

<p>我的理解是：</p>

<p>把原先tensor中的数据按照行优先的顺序排成一个一维的数据（这里应该是因为要求地址是连续存储的），然后按照参数组合成其他维度的tensor。比如说是不管你原先的数据是[[[1,2,3],[4,5,6]]]还是[1,2,3,4,5,6]，因为它们排成一维向量都是6个元素，所以只要view后面的参数一致，得到的结果都是一样的。比如，</p>

<p>a=torch.Tensor([[[1,2,3],[4,5,6]]])
b=torch.Tensor([1,2,3,4,5,6])</p>

<p>print(a.view(1,6))
print(b.view(1,6))</p>

<p>得到的结果都是tensor([[1., 2., 3., 4., 5., 6.]])</p>

</div>


    </main>

    
  </body>
</html>
