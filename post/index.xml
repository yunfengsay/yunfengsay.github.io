<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on 自留地</title>
    <link>https://yunfengsay.github.io/post/</link>
    <description>Recent content in Posts on 自留地</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Sat, 03 Nov 2018 23:22:09 +0800</lastBuildDate>
    
	<atom:link href="https://yunfengsay.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Linux_info</title>
      <link>https://yunfengsay.github.io/post/linux_info/</link>
      <pubDate>Sat, 03 Nov 2018 23:22:09 +0800</pubDate>
      
      <guid>https://yunfengsay.github.io/post/linux_info/</guid>
      <description>系统
# uname -a # 查看内核/操作系统/CPU信息 # head -n 1 /etc/issue # 查看操作系统版本 # cat /proc/cpuinfo # 查看CPU信息 # hostname # 查看计算机名 # lspci -tv # 列出所有PCI设备 # lsusb -tv # 列出所有USB设备 # lsmod # 列出加载的内核模块 # env # 查看环境变量  资源
# free -m # 查看内存使用量和交换区使用量 # df -h # 查看各分区使用情况 # du -sh &amp;lt;目录名&amp;gt; # 查看指定目录的大小 # grep MemTotal /proc/meminfo # 查看内存总量 # grep MemFree /proc/meminfo # 查看空闲内存量 # uptime # 查看系统运行时间、用户数、负载 # cat /proc/loadavg # 查看系统负载  磁盘和分区</description>
    </item>
    
    <item>
      <title>机器学习_01</title>
      <link>https://yunfengsay.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_01/</link>
      <pubDate>Wed, 31 Oct 2018 10:20:57 +0800</pubDate>
      
      <guid>https://yunfengsay.github.io/post/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_01/</guid>
      <description>pytorch squeeze(挤，压榨) 方法 除size为1的维度，包括行和列。至于维度大于等于2时，squeeze()不起作用。
batchsize 批大小。在深度学习中，一般采用SGD训练，即每次训练在训练集中取batchsize个样本训练；
iteration 1个iteration等于使用batchsize个样本训练一次；
epoch 1个epoch等于使用训练集中的全部样本训练一次；
举个例子，训练集有1000个样本，batchsize=10，那么： 训练完整个样本集需要： 100次iteration，1次epoch  learning rate 学习率 (learning rate)，控制 模型的 学习进度 ：
lr 即 stride (步长) ，即反向传播算法中的 ηη ：
ωn←ωn−η∂L∂ωn
   学习率 大 学习率 小      学习速度 快 慢   使用时间点 刚开始训练时 一定轮数过后   副作用 1.易损失值爆炸；2.易振荡。 1.易过拟合；2.收敛速度慢。    学习率设置 在训练过程中，一般根据训练轮数设置动态变化的学习率。
 刚开始训练时：学习率以 0.01 ~ 0.001 为宜。 一定轮数过后：逐渐减缓。 接近训练结束：学习速率的衰减应该在100倍以上。  Note： 如果是 迁移学习 ，由于模型已在原始数据上收敛，此时应设置较小学习率 (≤10−4≤10−4) 在新数据上进行 微调 。</description>
    </item>
    
    <item>
      <title>Jison</title>
      <link>https://yunfengsay.github.io/post/jison/</link>
      <pubDate>Mon, 15 Oct 2018 21:04:40 +0800</pubDate>
      
      <guid>https://yunfengsay.github.io/post/jison/</guid>
      <description>jison概述 什么是jison？
在聊jison前我们得先说下bison和flex这两个工具
Flex是一个词法分析器，是unix lex的gnu克隆，作用是把&amp;quot;词&amp;quot;抽象成符号，供程序识别 Bison则是一个文法分析器（也是unix yacc的gnu克隆），语法就是在这里定义，是语言设计的核心 这两巨头不但可以应付复杂的语法处理，也可以拿来制作简单的分析器，如配置文件等  而jison是bison的js实现（Jison generates bottom-up parsers in JavaScript）
jiosn的github地址在这里github，在examples下有很多作者写的demo教程。
jison做了什么 先不直接回答这个问题，我们从一个经典的需求说起 &amp;mdash; “实现一个计算器”
2*(5+2)/3  不是简单的调用下eval这么简单，我们要重头开始审视这个问题，优先级、四则符号、括号，这些就是我们要考虑的东西。
那么如何实现这个需求呢，拆分问题，我们人的思维是擅长把部分合成整体看待从而简化问题，首先拆分第一步
2*(5+2)/3 ----&amp;gt; 2*(5+2) / 3 相当于 let a = 22*(5+2); let b = 3 ; result = a / b (5+2)/3 ---&amp;gt; (5+2) / 3; 相当于 let c = (5+2) ; let d = 3; .....  最后计算， 详细逻辑可以参考别人的这篇文章 解析树及树的遍历
现在我们用jison来实现这个需求
第一步： 安装jison 新建 calculator.jison文件</description>
    </item>
    
  </channel>
</rss>